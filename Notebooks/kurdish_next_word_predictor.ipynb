{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "kurdish_next_word_predictor.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulbaseet-zahir/Kurdish-next-word-predictor/blob/main/Notebooks/kurdish_next_word_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20e7Vmh8tdEO",
        "outputId": "6fa82109-afdc-4152-93f3-d34b7766bfc1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyrNNzzqISSc"
      },
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from string import punctuation\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKLOBqsexAGt"
      },
      "source": [
        "dataset_file = \"/content/drive/MyDrive/kurdish next word prediction/AsoSoft mini-Text Corpus- Version 1.0 (2018-12-10).txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fCl_2ABug7L"
      },
      "source": [
        "def open_clean(dataset_file):\r\n",
        "    # output: open and clean text file then return all text as one string\r\n",
        "\r\n",
        "    with open(dataset_file, \"r\", encoding = \"utf8\") as f:\r\n",
        "        articles = [line for line in f]\r\n",
        "\r\n",
        "    \r\n",
        "    raw_data = ''\r\n",
        "    raw_data = ' '. join(articles)\r\n",
        "    \r\n",
        "    data = raw_data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\r\n",
        "    data = re.sub(r'[^\\w\\s]','', data)\r\n",
        "\r\n",
        "    return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIEm-TtxtdF"
      },
      "source": [
        "data = open_clean(dataset_file)[:10000]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6zjdaKqH148"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts([data])\r\n",
        "pickle.dump(tokenizer, open('/content/drive/MyDrive/kurdish next word prediction/bigtokenizer.pkl', 'wb'))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y__--m5TPi9r"
      },
      "source": [
        "sequence_data = tokenizer.texts_to_sequences([data])[0]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjVVHjboISSq"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "X1 = []\n",
        "y1 = []\n",
        "\n",
        "for i in sequences:\n",
        "    X1.append(i[0])\n",
        "    y1.append(i[1])\n",
        "    \n",
        "X = np.array(X1)\n",
        "Y = np.array(y1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US0M1dcfNdeu",
        "outputId": "c7f1c671-82b4-41a9-eff9-c2ab05c30397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA8es0BCISSr"
      },
      "source": [
        "def data_gen(ins, outs, vocab_size, batch_size=100):\r\n",
        "    \r\n",
        "    while True:\r\n",
        "        x = ins[i:batch_size]\r\n",
        "        y = to_categorical(outs[i:batch_size], num_classes=vocab_size)\r\n",
        "        \r\n",
        "        batch_size=100+batch_size\r\n",
        "        yield x, y\r\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlIpjOxKISSr"
      },
      "source": [
        "model = tensorflow.keras.models.Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "current_time = str(int(time.time()))\n",
        "current_time2 = '_'.join(str(time.asctime()).split())\n",
        "current_time3 = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='auto')\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "tensorboard_Visualization = TensorBoard('logs\\\\'+current_time3, histogram_freq=1)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfRzQuSOagw"
      },
      "source": [
        "g = data_gen(X,Y,vocab_size)\r\n",
        "model.fit_generator(g, 10, epochs=150 , callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuRX9Dd2i8fl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UVA6X0vnISSt"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "string_word = 'کەناکرێت هەروا بەئاسانی تێیان بڕوانین بەڵکو بۆبکرێت خەڵکی شیاویان بۆبدۆزرێتەوە بەلام وەخت لەمیزاج تەکەتول دووربکەوینەوە هەنگاونان لێرەدا ئەوپەڕی دیموکراتیەت ئازادی بەخۆوە ببینێت کەسی شیاو بەدەنگی بچێتە جێیە بەتالانە ئەوەش لەبەرچاو بگرین دەیان سەرکردەی شەهیدبوون چەندین جار ئەوە هاتووە شوێنی سەرکردەکانی چۆڵ بن دواتر پڕبکرێنەوە لەزۆربەی کاتەکان بەرزکردنەوەی کەسێک یان کەسانێک بۆپڕکردنەوەی شوێنە بەتاڵەکان کراوە بۆیە زۆرجار دەرنەچووە کەچاوەڕوان واتە بەرزکردنەوە کەمتر لە خۆدەگرێت لەوەی هەڵبژاردنی پەیڕەوکردنی سزاو پاداشت گۆڕانە بەخۆیەوە دەبینێت دەستبەکار کەبەشێکی گرنگی کابینەیە پێکدەهێنێ گرێ دەدات بۆرێکخستنەوەی ماڵی یەکێتی لەگشت لایەن کەسەکان ڕووداوە بەگرنگ بخوێنێتەوە پلینۆمیش تەنیا سەرخستن جێ پڕکردنەوەو جێگۆڕکێ نەبێت بۆپەیڕەوکردنی پاداشتیش کەکاری خراپیان کردووە باشیان بگیرێت کەئەو کەسانەی نەیان توانیوە کارەکانیان جێبەجێبکەن بکەن بۆهەڤالانی هاتنەپێشەوە قولایی سیاسییەکان دوێنێ 31 10 2009 سێیەمین دانیشتنی یەکێتیی نیشتمانیی لەتەلاری هونەر لەسلێمانی بەڕێوەچوو تێیدا ڕاپۆرتی لیژنەی ناوچەکانی لەلایەن ڕزگار عەلی خوێنرایەوە ئاماژەیکرد بۆئەو ناوچانە کەپارێزگای کەرکوک سنووری نەینەواو دیالەو سەلاحەددین واست بەغدا دەگرێتەوە مێژووی سیاسەتی مەترسیداری تەعریب تەهجیرو دابڕاندنی ناوچانەی خستەڕوو پەنجەشی خستەسەر کێشەکانی دانیشتووانی بەدەست زوڵمی یاسایی ئەمنی ئابوورییەوە دەناڵێنن ئاسانکاری کێشەکان گوتیشی ماددەی 140 ی دەستوور ماددەیەکی دەستووریی UN گەڕایەوەسەر پارێزگای هەولێر تێبینی ڕەخنەکانی ئەندامان لەپێناوی زیاتر دەوڵەمەندکردنی کەگفتوگۆیەکی هەمەلایەنەی لەبارەوە کراو پەرۆشیی بەرجەستەدەکرد بەگەڕانەوەی جێناکۆک جێبەجێکردنی دەستووری جێناکۆکی لەوبارەیەشەوە مەلا بەختیار ئەندامێکی خانەقین خستنەڕووی هۆکارەکانی سستی داوایکرد شەڕی مەرکەزی ناکۆکییەکان کەشەڕی کورد لەدەرەوەی شارەکانی کێشە بەخزمەتکردن خزمەتگوزاری ئەولەویەت بدات بەسنووری وەزارەتێکی کارەی نەکرد لەبەشەکەی متمانەی لێوەربگیرێتەوە شێخ جەعفەر مستەفاش وێڕای پڕۆژەیەک پاراستنی ئاسایشی دووپاتیکردەوە کەئەگەر پڕۆژەیە جێبەجێنەکرا پێشمەرگە شوێنێک لەو دەمێنێتەوەو هیچ ناکات پاشان بەتێکڕای دەنگ پەسەندکرا سکالاو گازندەکان پەسەندکردنی حاکم قادر حەمەجان عەزیزەوە کەچەندین خاڵی تێدا خرابووەڕوو هەم وەکو گازەندەو ڕەخنە ڕاسپاردە زیندووێتی ژیانی ڕێکخراوەیی خالانەدا پابەندبوون بەپەیڕەوی ناوخۆی نەهێشتنی ناعەدالەتی ڕێگەگرتن لەزیادەڕۆیی دەسەلاتی هەندێک لەلێپرسراوان دووپاتکرابووەوە پێشنیازی گرنگ واقیعیش خرانەڕوو کەهەمووی بەئاراستەی بەهێزکردن گوڕوتین بەخشین بوو ئاماژەش کرابوو بەوەی کەزیاتر بایەخ بەژنان پێگەی ژن گەنجان کەسوکاری شەهیدان ئەنفالکراو کەمئەندامانی سەنگەر بدرێت یاداشتکرا کەیاسای کادیران ڕێکبخرێت بنەماو ڕێساکان کاریش بەپاداشت سزا ڕوونکرابووەوە کەسنوورێک دەستوەردانی لێپرسراوان لەبازرگانی کۆکردنەوەی سامان دابنرێت بنبڕکردنی دەستەگەری دەربڕینی وردی موناقەشەی دڵسۆزانە ڕەخنەی بنیاتنەر دروستکەری بەهێز هەروەها بەرلەوەی لیژنەکە بخرێتە دەنگدانەوە کۆسرەت ڕەسوڵ داوای چەند دەقیقەیەک دەرفەتی قسەکردنی کردو ڕێگەپێدانی گوتەیەکی پێشکەشکرد تەکەتولی دووپاتکردەوە داواشیکرد بەزووترین کات نەزاهە پێکبهێنرێت زانینی سەروەت سامانی داواشی لەئەندامانی ڕیزەکانی ڕێکخستن کرد کەیەکێتیی بپارێزن بەناوی سەرکردایەتیشەوە بەڵێنیدا کەپابەندی گفتانەبن کەدراوەو دەدرێت گوتی جێبەجێنەکران لەکۆنگرەدا سەرکردایەتییە هەڵمەبژێرن خرایە دەنگی پەیامی لەڕاگەیاندنی بڕیاری پێکهێنانی ڕاگەیاندن پێکهات لەوادەی دیاریکراوی خۆیدا کۆ بووەوە لەسێیەم ڕۆژی پلینۆمدا ئازاد جندیانی ڕاپۆرتەکەی خوێندەوە کۆنفرانسەی ڕاگەیاندنی لەناوەڕاستی مانگی ئۆکتۆبەردا بەسترا پێشخستن کرانەوەی تێبینییەکانی لیژنەکەی نەبوونی پەیام کەپەرشوبلاوی لەڕاگەیاندنەکاندا دروستکردووە کورتخایەن درێژخایەن گەشەپێدانی لەوبارەیەوە ڕوونکراوانەی لەنێو ڕاپۆرتەکەدا لەلیژنەکەدا پێشکەشیکردبوو ڕوونیکردەوە بەرجەستەیە پەیامیشی گەڕاندەوە لەمەکتەبی سیاسیدا درێژخایەنیش پلانە لەهەموو مەکتەب ئۆرگانەکانی کورستاندا پەیڕەوبکرێت ڕاو بۆچوونی تێروتەسەل بەزۆرینەی بایەخدانی بەکەسوکاری لەدانیشتنی دوانیوەڕۆی سێیەم لیژنەیەکی پەسەندکران لەدانیشتنەکەدا بۆردومانی کیمیایی ئەنفال وارسەکانیان سەنگەرو زیندانیانی پێشکەشکران عەبدولکەریم هەڵەدنی خوێندیەوە پڕۆژە لەخۆگرتبوو لەبەرژەوەندیی کەیسانە گرنگیدان بەوارسی قوربانیانی چەکی تێکڕای خانووبەرە خوێندنی کوڕان کچانی شەهید وارسی تاچڕکردنەوەی هەوڵەکان کۆمەڵکوژی لەجیهاندا بەجینۆساید بناسرێت دروستکردنی مۆنیومێنت هێنانەوەی ڕوفاتی ئەنفالکراوەکان پێشنیازو لەڕاپۆرتی لیژنەکەدا قەرەبووبکرێنەوەو ژیانێکی شایستەیان دابینبکرێت کەبارتەقای قوربانیەکانیان تەئکیدکرایەوە کەدەبێت تێکۆشەرانی کۆچکردوو پۆلێنبکرێن تاوەکو ئەوانیش ژیانیان پڕۆژەکە لەبارەوەکراو بەئازادی ڕاشکاوی تێبینییەکانیان داکۆکییان لەمافی ڕەخنەش کەموکوڕیانە گیرا کەدەرهەق بەلایەنە پەیوەستەکانی کەیسەوە لەکۆتاییشدا مەکتەبەکانی لەڕاپۆرتەکەدا ئاماژە کرا ڕێکخراوە دێموکراتییەکان کەهەوڵبدات یەکخستنەوەی ڕێکخراوو کۆمەڵەکان گوێدانە ئینتیمای هاوئاهەنگیی نێوان داوایەکی لیژنەیەبوو لەکەموکورتییەکان داوایکردبوو کارئاسانی چاودێری ئۆرگانەکان بەسیستمی دامەزراوەیی لەکەموکورتی لەمەکتەبەکان مەکتەبەکان پەیڕەوی ناوخۆیان'\n",
        "list_words = string_word.split()\n",
        "\n",
        "model = load_model('nextword2.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "for word in list_words:\n",
        "    \n",
        "    try:\n",
        "        seq = np.array(tokenizer.texts_to_sequences([word])[0])\n",
        "        predd = model.predict_classes(seq)\n",
        "\n",
        "        for key, value in tokenizer.word_index.items():\n",
        "                    if value == predd:\n",
        "                        predicted_word = key\n",
        "                        break\n",
        "        print('seq ->',str(tokenizer.index_word[int(seq)]))\n",
        "        print('key ->',key)\n",
        "        print('value ->',value)\n",
        "        print('predicred ->', predicted_word)\n",
        "        print('')\n",
        "    except: continue\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwKLW56pISSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0lAdiGEISSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwPJw7U8ISSu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWG9977uISSv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhC_xOSrISSv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU4eHNCAISSv"
      },
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword2.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "#         print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                \n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        \n",
        "        return predicted_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Nj1HRUKZISSv",
        "outputId": "f7fad061-9816-4fea-c764-69f9349b2ccf"
      },
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your line: هەروا\n",
            "Enter your line: stop\n",
            "Ending The Program.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ckF1vJISSv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj5tq_QbISSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C58nKSA0ISSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nLqdCeqISSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9nHGZhISSx"
      },
      "source": [
        "model2 = model.save('my_model2.h5', )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqJSHzeHISSx"
      },
      "source": [
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEnRxcrBISSx"
      },
      "source": [
        "seq = np.array(tokenizer.texts_to_sequences(['و'])[0])\n",
        "predd = model.predict(seq)\n",
        "predd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK-Rb4rVISSx"
      },
      "source": [
        "for key, value in tokenizer.word_index.items():\n",
        "            if (value == predd).all():\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "print(predicted_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N6GFLrnISSx"
      },
      "source": [
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "#         print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9d2n0_zISSx"
      },
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}