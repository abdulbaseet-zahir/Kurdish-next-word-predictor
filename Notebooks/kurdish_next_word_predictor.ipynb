{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "kurdish_next_word_predictor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulbaseet-zahir/Kurdish-next-word-predictor/blob/main/Notebooks/kurdish_next_word_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20e7Vmh8tdEO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi8f_2TAbeVQ"
      },
      "source": [
        "%cd /content/drive/MyDrive/kurdish next word prediction/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyrNNzzqISSc"
      },
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from string import punctuation\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKLOBqsexAGt"
      },
      "source": [
        "dataset_file = \"kurdish text data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fCl_2ABug7L"
      },
      "source": [
        "def open_and_clean(dataset_file):\n",
        "    'Open and clean text file then return all text as one string'\n",
        "\n",
        "    with open(dataset_file, \"r\", encoding = \"utf8\") as f:\n",
        "        articles = [line for line in f]\n",
        "\n",
        "    \n",
        "    raw_data = ''\n",
        "    raw_data = ' '. join(articles)\n",
        "    \n",
        "    data = raw_data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "    data = re.sub(r'[^\\w\\s]','', data)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIEm-TtxtdF"
      },
      "source": [
        "data = open_and_clean(dataset_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6zjdaKqH148"
      },
      "source": [
        "def data_to_seq(data):\n",
        "    'Tokenize the data to feed to model and save a pkl file for later use'\n",
        "    'Convert each word as sequenced number' \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([data])\n",
        "    pickle.dump(tokenizer, open('data_tokenizer.pkl', 'wb'))\n",
        "    token_dic = tokenizer.word_index\n",
        "    seq_data = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "    return token_dic, seq_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgDbOMaEgRtb"
      },
      "source": [
        "tokenizer_dic, sequence_data = data_to_seq(data)\n",
        "vocab_size = len(tokenizer_dic)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjVVHjboISSq"
      },
      "source": [
        "def get_X_Y(seq_data):\n",
        "    'Return two list of sequenced words, X for input words Y for its next'\n",
        "    sequences = []\n",
        "\n",
        "    for i in range(1, len(sequence_data)):\n",
        "        #Each time creat a list of two elemnts the first one is a word and second the word after of it\n",
        "        words = sequence_data[i-1:i+1]\n",
        "        sequences.append(words)\n",
        "\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "    X1 = []\n",
        "    y1 = []\n",
        "\n",
        "    for i in sequences:\n",
        "        X1.append(i[0])\n",
        "        y1.append(i[1])\n",
        "        \n",
        "    X = np.array(X1)\n",
        "    Y = np.array(y1)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US0M1dcfNdeu"
      },
      "source": [
        "X, Y = get_X_Y(sequence_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4gcaEQPqwm2"
      },
      "source": [
        "#This Generator use for larg datasets, to feed the model batch by batch\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, inputs, labels, vocab_size, batch_size=32, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.inputs = inputs\n",
        "        self.vocab_size = vocab_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.inputs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of Inputs\n",
        "        list_inputs_temp = [self.inputs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_inputs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.inputs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_inputs_temp):\n",
        "        'Generates data containing batch_size samples'\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size,))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, input in enumerate(list_inputs_temp):\n",
        "            # Shuffled inputs\n",
        "            X[i,] = input\n",
        "\n",
        "            # Shuffled labels\n",
        "            y[i] = self.labels[input]\n",
        "\n",
        "        return X, to_categorical(y, num_classes=self.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "805hFVAImFzD"
      },
      "source": [
        "# Creating the LSTM model\n",
        "model = tensorflow.keras.models.Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlIpjOxKISSr"
      },
      "source": [
        "# Make sure we have Checkpoints\n",
        "checkpoint = ModelCheckpoint(\"nextword.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='auto')\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "tensorboard_Visualization = TensorBoard(\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EatTxeFWqx_n"
      },
      "source": [
        "training_generator = DataGenerator(X, Y, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYiWLqtbsI9b"
      },
      "source": [
        "model.fit(training_generator, epochs=20, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANL6tkPQkLf7"
      },
      "source": [
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxFVJzhSpurC"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UVA6X0vnISSt"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "string_word = open_and_clean(\"for model testing.txt\")\n",
        "\n",
        "list_words = string_word.split()\n",
        "\n",
        "model = load_model('nextword.h5')\n",
        "tokenizer = pickle.load(open('data_tokenizer.pkl', 'rb'))\n",
        "\n",
        "for word in list_words:\n",
        "    \n",
        "    try:\n",
        "        seq = np.array(tokenizer.texts_to_sequences([word])[0])\n",
        "        predd = model.predict_classes(seq)\n",
        "\n",
        "        for key, value in tokenizer.word_index.items():\n",
        "                    if value == predd:\n",
        "                        predicted_word = key\n",
        "                        break\n",
        "        print('input ->',str(tokenizer.index_word[int(seq)]))\n",
        "        print('predicred ->', predicted_word)\n",
        "        print('')\n",
        "    except: continue\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}